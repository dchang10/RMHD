{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/github/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "\n",
    "# Lectures on Machine Learning for Strong Gravity\n",
    "## Lecture 1: Physics-Informed Neural Networks\n",
    "\n",
    "To test it, simply press Ctrl+Enter sequentially in each cell, or click on the small icons on the left with the \"play\" symbol.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### In this lecture, you will learn:\n",
    "1. How to use PINNs to solve differential equations, including ODEs, PDEs and eigenvalue problems.<br>Original paper:<br>\n",
    "\n",
    " - Maziar Raissi, Paris Perdikaris, George Em Karniadakis, <i>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</i>, Journal of Computational Physics, 378, 686-707, 2019. https://arxiv.org/abs/1711.10561\n",
    " \n",
    "2. How these can be used, for instance, for the computation of QNMs.<br>Original paper:<br>\n",
    "\n",
    " - Raimon Luna, Juan Calderón Bustillo, Juan José Seoane Martínez, Alejandro Torres-Forné, José A. Font, <i>Solving the Teukolsky equation with physics-informed neural networks\n",
    "</i>, Phys.Rev.D 107 (2023) 6, 064025, 2023. https://arxiv.org/abs/2212.06103\n",
    "\n",
    "<br>\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(outputs, inputs, order = 1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1) # Recursively take gradients\n",
    "    else:\n",
    "        return outputs\n",
    "\n",
    "def generate_2Dgrid(minimum1, maximum1, minimum2, maximum2, N):\n",
    "    grid1 = np.linspace(minimum1, maximum1, N, dtype = np.float32)\n",
    "    grid2 = np.linspace(minimum2, maximum2, N, dtype = np.float32)\n",
    "    x0, y0 = np.meshgrid(grid1, grid2)\n",
    "    x = torch.tensor(x0.reshape(N**2), requires_grad = True)\n",
    "    y = torch.tensor(y0.reshape(N**2), requires_grad = True)\n",
    "    return x, y\n",
    "\n",
    "def plot_form(x, y, z, N):\n",
    "    return map(lambda t: t.reshape(N,N).cpu().detach().numpy(), (x, y, z))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"# 1. Automatic differentiation and optimization\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.linspace(-2*np.pi, 2*np.pi, 100, dtype = np.float32)).reshape(100, 1) # list of xvalues to evaluate on\n",
    "x.requires_grad = True\n",
    "\n",
    "y = torch.sin(x) # sin(x)\n",
    "\n",
    "dy_dx = gradients(y, x)\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.plot(x.detach().numpy(), dy_dx.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.5], requires_grad = True)\n",
    "y = torch.tensor([0.5], requires_grad = True)\n",
    "\n",
    "track = []\n",
    "\n",
    "#ADAM optimization algorithm \n",
    "#https://towardsdatascience.com/complete-guide-to-adam-optimization-1e5f29532c3d#:~:text=1.-,Definition%20of%20Adam%20Optimization,memory%20requirement%E2%80%9D%20%5B2%5D.\n",
    "optimizer = optim.Adam([x, y], lr=0.1)\n",
    "\n",
    "# Use ADAM to traverse the space defined by exp(-(x + y)**2 - y**2)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = 1 - torch.exp( -(x + y)**2 - y**2  )\n",
    "    z.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    track.append([x.item(), y.item()])\n",
    "\n",
    "track = np.array(track)\n",
    "\n",
    "x0, y0 = np.meshgrid(np.linspace(-2,2,100), np.linspace(-2,2,100))\n",
    "plt.contourf(x0, y0, 1 - np.exp( -(x0 + y0)**2 - y0**2  ))\n",
    "plt.scatter(track[:, 0], track[:, 1], color = 'red', marker='x')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges!\n",
    " - Try computing derivatives of other functions\n",
    " - Try changing the learning rate. What happens if it gets too small? Or too big?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import scipy.special as sp\n",
    "x = torch.tensor(np.linspace(-2*np.pi, 2*np.pi, 100, dtype = np.float32)).reshape(100, 1) # list of xvalues to evaluate on\n",
    "x.requires_grad = True\n",
    "\n",
    "y =  sp.ellipk(x)# sin(x)\n",
    "dy_dx = torch.autograd.grad(y, x, torch.ones_like(y), create_graph=True)[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.linspace(-2*np.pi, 2*np.pi, 100, dtype = np.float32)).reshape(100, 1) # list of xvalues to evaluate on\n",
    "x.requires_grad = True\n",
    "\n",
    "y = x**2# sin(x)\n",
    "\n",
    "dy_dx = gradients(y, x)\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.plot(x.detach().numpy(), dy_dx.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Solving a system of ODEs: Sine and Cosine\n",
    "\n",
    "Here we solve the system of ODEs\n",
    "\n",
    "$$s'(x) = c(x),$$\n",
    "$$c'(x) = -s(x),$$\n",
    "\n",
    "with the initial conditions\n",
    "\n",
    "$$s(0) = 0, \\;c(0) = 1.$$\n",
    "\n",
    "We impose the initial conditions by weak enforcement.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ODE solution will be modelled as a NN with 1 hidden layer of 40 nodes \n",
    "# and output 2 values\n",
    "class ODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODE, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# The loss function that will be used to train the NN will be constructed\n",
    "# out of the DE\n",
    "class ODELoss(nn.Module):\n",
    "    def __init__(self, ode):\n",
    "        super(ODELoss, self).__init__()\n",
    "        self.ode = ode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        funcs  = self.ode(x)\n",
    "        \n",
    "        c, s = map(lambda i:  funcs[:,[i]], range(2))\n",
    "        dc = gradients(c, x)\n",
    "        ds = gradients(s, x)\n",
    "        \n",
    "        zero_vals = self.ode(torch.zeros(1,1))\n",
    "\n",
    "        # loss function associated with satisfying DE        \n",
    "        eq_loss = torch.mean((dc + s)**2 + (ds - c)**2)\n",
    "\n",
    "        #loss function that constrains Initial Conditions\n",
    "        ic_loss = (zero_vals[0,0] - 1)**2 + zero_vals[0,1]**2\n",
    "        \n",
    "        return eq_loss, ic_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE()\n",
    "odeloss = ODELoss(ode)\n",
    "loss_hist = []\n",
    "\n",
    "optimizer = optim.Adam(ode.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.05)**(1/4000))\n",
    "\n",
    "x = torch.tensor(np.linspace(0, 10, 100, dtype = np.float32)).reshape(100, 1)\n",
    "x.requires_grad = True\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 4))\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(4000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        eq_loss, ic_loss = odeloss(x)\n",
    "        loss = eq_loss + ic_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0: #plot every 100 epochs\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "\n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('y')\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x)[:, 0].cpu().detach().numpy())\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x)[:, 1].cpu().detach().numpy())\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge!\n",
    "\n",
    " - Try imposing the conditions at $x = 2\\pi$ instead of at $x = 0$. Does it affect the performance?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODELoss2(nn.Module):\n",
    "    def __init__(self, ode):\n",
    "        super(ODELoss2, self).__init__()\n",
    "        self.ode = ode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        funcs  = self.ode(x)\n",
    "        \n",
    "        c, s = map(lambda i:  funcs[:,[i]], range(2))\n",
    "        dc = gradients(c, x)\n",
    "        ds = gradients(s, x)\n",
    "        \n",
    "        twopi_vals = self.ode(torch.asarray([[2*np.pi]]))\n",
    "\n",
    "        # loss function associated with satisfying DE        \n",
    "        eq_loss = torch.mean((dc + s)**2 + (ds - c)**2)\n",
    "\n",
    "        #loss function that constrains Initial Conditions\n",
    "        ic_loss = (twopi_vals[0,0] - 1)**2 + twopi_vals[0,1]**2\n",
    "        \n",
    "        return eq_loss, ic_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE()\n",
    "odeloss = ODELoss2(ode)\n",
    "loss_hist = []\n",
    "\n",
    "optimizer = optim.Adam(ode.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.05)**(1/4000))\n",
    "\n",
    "x = torch.tensor(np.linspace(0, 10, 100, dtype = np.float32)).reshape(100, 1)\n",
    "x.requires_grad = True\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 4))\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(4000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        eq_loss, ic_loss = odeloss(x)\n",
    "        loss = eq_loss + ic_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0: #plot every 100 epochs\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "\n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('y')\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x)[:, 0].cpu().detach().numpy())\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x)[:, 1].cpu().detach().numpy())\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASIDE: Solving the Inviscid Burgers equation\n",
    "\n",
    "Here we solve the inviscid Burgers Equation\n",
    "\n",
    "$$\\partial_t u(t,x) = u(t,x) \\partial_x u(t,x),$$\n",
    "\n",
    "with the initial conditions\n",
    "\n",
    "$$u(0,x) = f(x) = \\begin{cases}1 && \\text{if }x < 0\\\\1-x && \\text{if }0\\leq x \\leq 1\\\\0 && \\text{if }1< x \\\\\\end{cases}$$\n",
    "\n",
    "We impose the initial conditions by weak enforcement.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    if x < -1:\n",
    "        return 0\n",
    "    elif x < 0:\n",
    "        return x+1\n",
    "    elif x < 1:\n",
    "        return 1-x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# The ODE solution will be modelled as a NN with 1 hidden layer of 40 nodes \n",
    "# and output 2 values\n",
    "class InviscidBurgers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InviscidBurgers, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 40),\n",
    "            nn.Tanh(),\n",
    "            #nn.Linear(40, 40),\n",
    "            #nn.Tanh(),\n",
    "            nn.Linear(40, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "\n",
    "    def hard_enforcement(self, t, x, u):\n",
    "        return torch.mul(-(x**2-4), torch.mul(torch.exp(t)-1, u)) + x/torch.abs(x)#+ torch.asarray(list(map(g, x))), u)\n",
    "            \n",
    "    def forward(self, t, x):\n",
    "        tx = torch.stack((t,x), 1)\n",
    "        #bdry = (abs(x) - 1).reshape(len(x), 1)\n",
    "        #return self.net(tx) \n",
    "        outnet = self.net(tx)\n",
    "        return torch.mul((((torch.exp(2*t)-1)).reshape(len(t),1)), outnet) + (torch.asarray(list(map(g, x)))).reshape(len(t),1)\n",
    "\n",
    "# The loss function that will be used to train the NN will be constructed\n",
    "# out of the DE\n",
    "\n",
    "class InviscidBurgersLoss(nn.Module):\n",
    "    def __init__(self, approx_sol):\n",
    "        super(InviscidBurgersLoss, self).__init__()\n",
    "        self.approx_sol = approx_sol\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "        u   =  self.approx_sol(t, x)\n",
    "        u_t = gradients(u, t, 1)\n",
    "        u_x = gradients(u, x, 1)\n",
    "\n",
    "        eq = u_t +u* u_x\n",
    "\n",
    "        # loss function associated with satisfying DE        \n",
    "        eq_loss = abs(eq)\n",
    "\n",
    "        return torch.max(eq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ODE solution will be modelled as a NN with 1 hidden layer of 40 nodes \n",
    "# and output 2 values\n",
    "class InviscidBurgers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InviscidBurgers, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, t, x):\n",
    "        tx = torch.stack((t,x), 1)\n",
    "        #bdry = (abs(x) - 1).reshape(len(x), 1)\n",
    "        return self.net(tx) #* bdry\n",
    "\n",
    "# The loss function that will be used to train the NN will be constructed\n",
    "# out of the DE\n",
    "def g(x):\n",
    "    if x < -1:\n",
    "        return 0\n",
    "    elif x < 0:\n",
    "        return x+1\n",
    "    elif x < 1:\n",
    "        return 1-x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "resolution = 1000\n",
    "class InviscidBurgersLoss(nn.Module):\n",
    "    def __init__(self, approx_sol):\n",
    "        super(InviscidBurgersLoss, self).__init__()\n",
    "        self.approx_sol = approx_sol\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "\n",
    "       # global test1, test2, test3, test4\n",
    "      \n",
    "        u   =  self.approx_sol(t, x).squeeze()\n",
    "        u_t = gradients(u, t)\n",
    "        u_x = gradients(u, x)\n",
    "\n",
    "        eq = u_t + u* u_x\n",
    "\n",
    "        # loss function associated with satisfying DE        \n",
    "        eq_loss = (eq)**2\n",
    "\n",
    "\n",
    "\n",
    "        #define the initial values of t,x\n",
    "        tinitial=torch.zeros(resolution)\n",
    "        xinitial=torch.as_tensor([x[resolution*i] for i in range(resolution)])\n",
    "\n",
    "        #pde(torch.zeros(30),xvalues)-torch.as_tensor(list(map(g, xvalues)))\n",
    "\n",
    "        ic = torch.flatten(self.approx_sol(tinitial,xinitial) )#self.approx_sol(torch.zeros(x.detach().numpy().size), x)\n",
    "        \n",
    "\n",
    "        #loss function that constrains Initial Conditions\n",
    "        #ic_loss = map(lambda elem : (elem[0]-1)**2 if elem[1] < -1 else (elem[0] - (1-elem[1]))**2 if elem[1] < 1 else x[0]**2, zip(ic, x))\n",
    "        ic_loss =(ic - torch.as_tensor(list(map(g, xinitial))))**2\n",
    "        \n",
    "       # test1=eq_loss\n",
    "       ## test3=u_x\n",
    "        #test2= ic_loss\n",
    "        #test4=u*u_x\n",
    "\n",
    "        return torch.mean(eq_loss)+2*resolution*torch.sum(ic_loss) #torch.mean(torch.cat((eq_loss, 2*ic_loss),dim=0)) #torch.mean(eq_loss.append() ) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde      = InviscidBurgers()\n",
    "pde_loss = InviscidBurgersLoss(pde)\n",
    "loss_hist = []\n",
    "\n",
    "t, x = generate_2Dgrid(0, 3, -2, 2, resolution)\n",
    "optimizer = optim.Adam(pde.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.1)**(1/5000))\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "p = None\n",
    "\n",
    "for it in range(5000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = pde_loss(t, x)\n",
    "        loss_hist.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "        if  it %100 == 0:\n",
    "            t_plot, x_plot, u_plot = plot_form(x, t, pde(t, x), resolution)\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "            \n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('t')\n",
    "            p = ax2.contourf(t_plot, x_plot, u_plot)\n",
    "            \n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "fig.colorbar(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pde(torch.ones(50), torch.linspace(-2,2,50))\n",
    "fig, (ax1) = plt.subplots(1,1, figsize = (12, 5));\n",
    "ax1.plot(torch.linspace(-2,2,50), u.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASIDE: MHD\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ODE solution will be modelled as a NN with 1 hidden layer of 40 nodes \n",
    "# and output 2 values\n",
    "gamma = 2.0\n",
    "Bx = 0.75\n",
    "resolution = 100\n",
    "class MHD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MHD, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 7),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, t, x):\n",
    "        tx = torch.stack((t,x), 1)\n",
    "        #bdry = (abs(x) - 1).reshape(len(x), 1)\n",
    "        return self.net(tx) #* bdry\n",
    "\n",
    "# The loss function that will be used to train the NN will be constructed\n",
    "# out of the DE\n",
    "def initial_condition(x):\n",
    "    if x < 0:\n",
    "        return [1.0,0.0,0.0,0.0,1.0,0.0,1.0]\n",
    "    else:\n",
    "        return [0.125,0.0,0.0,0.0,-1.0,0.0,0.1]\n",
    "\n",
    "\n",
    "def conserved(P):\n",
    "    t_P = P.t()\n",
    "    rho = t_P[0]\n",
    "    vx = t_P[1]\n",
    "    vy = t_P[2]\n",
    "    vz = t_P[3]\n",
    "    By = t_P[4]\n",
    "    Bz = t_P[5]\n",
    "    p = t_P[6]\n",
    "\n",
    "    rhovx = vx*rho\n",
    "    rhovy = vy*rho\n",
    "    rhovz = vz*rho\n",
    "\n",
    "    energy = (p/(gamma-1) + 1/2*(Bx**2+By**2+Bz**2) + rho/2*(vx**2+vy**2+vz**2))\n",
    "\n",
    "    return torch.stack((rho, rhovx, rhovy, rhovz, By, Bz, energy)).t()\n",
    "    #return torch.tensor(P)\n",
    "\n",
    "def current(P):\n",
    "    t_P = P.t()\n",
    "    rho = t_P[0]\n",
    "    vx = t_P[1]\n",
    "    vy = t_P[2]\n",
    "    vz = t_P[3]\n",
    "    By = t_P[4]\n",
    "    Bz = t_P[5]\n",
    "    p = t_P[6]\n",
    "\n",
    "    rhovx = vx*rho\n",
    "\n",
    "    ps = p + 1/2*(Bx**2+By**2+Bz**2)\n",
    "    energy = (p/(gamma-1) + 1/2*(Bx**2+By**2+Bz**2) + rho/2*(vx**2+vy**2+vz**2))\n",
    "\n",
    "    return torch.stack((\n",
    "        rhovx, \n",
    "        rhovx**2 + ps -Bx**2, \n",
    "        rhovx*vy - Bx*By, \n",
    "        rhovx*vz - Bx*Bz, \n",
    "        By*vx - Bx*vy, \n",
    "        Bz*vx - Bx*vz, \n",
    "        (energy+ps)*vx - Bx*(Bx*vx+By*vy+Bz*vz)\n",
    "    )).t()\n",
    "\n",
    "class MHDLoss(nn.Module):\n",
    "    def __init__(self, approx_sol):\n",
    "        super(MHDLoss, self).__init__()\n",
    "        self.approx_sol = approx_sol\n",
    "        self.conserved = conserved\n",
    "        self.current = current\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "\n",
    "       # global test1, test2, test3, test4\n",
    "      \n",
    "        P = self.approx_sol(t, x)#.squeeze()\n",
    "        U = self.conserved(P)\n",
    "        J = self.current(P)\n",
    "\n",
    "        eq = gradients(U, t) + gradients(J, x)\n",
    "\n",
    "        # loss function associated with satisfying DE        \n",
    "        eq_loss = (torch.flatten(eq))**2\n",
    "\n",
    "        #define the initial values of t,x\n",
    "        tinitial=torch.zeros(resolution)\n",
    "        xinitial=torch.as_tensor([x[resolution*i] for i in range(resolution)])\n",
    "\n",
    "        approx_sol_ic = self.approx_sol(tinitial,xinitial) #self.approx_sol(torch.zeros(x.detach().numpy().size), x)\n",
    "        \n",
    "\n",
    "        #loss function that constrains Initial Conditions\n",
    "        #ic_loss = map(lambda elem : (elem[0]-1)**2 if elem[1] < -1 else (elem[0] - (1-elem[1]))**2 if elem[1] < 1 else x[0]**2, zip(ic, x))\n",
    "        ic_loss =(approx_sol_ic - torch.as_tensor(list(map(initial_condition, xinitial))))**2\n",
    "\n",
    "        return torch.mean(eq_loss)+2*resolution*torch.sum(ic_loss) #torch.mean(torch.cat((eq_loss, 2*ic_loss),dim=0)) #torch.mean(eq_loss.append() ) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde      = MHD()\n",
    "pde_loss = MHDLoss(pde)\n",
    "loss_hist = []\n",
    "\n",
    "t, x = generate_2Dgrid(0, 0.2, -1, 1, resolution)\n",
    "optimizer = optim.Adam(pde.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.1)**(1/5000))\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "p = None\n",
    "\n",
    "for it in range(5000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = pde_loss(t, x)\n",
    "        loss_hist.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "        if  it %100 == 0:\n",
    "            t_plot, x_plot, u_plot = plot_form(x, t, pde(t, x).t()[0], resolution)\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "            \n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('t')\n",
    "            p = ax2.contourf(t_plot, x_plot, u_plot)\n",
    "            \n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "fig.colorbar(p)\n",
    "#hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pde(0.2*torch.ones(resolution), torch.linspace(-1,1,resolution))\n",
    "fig, (ax1) = plt.subplots(1,1, figsize = (12, 5));\n",
    "ax1.plot(torch.linspace(-2,2,resolution), u.t().detach().numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pde(0.2*torch.zeros(resolution), torch.linspace(-1,1,resolution))\n",
    "fig, (ax1) = plt.subplots(1,1, figsize = (12, 5));\n",
    "ax1.plot(torch.linspace(-2,2,resolution), u.t().detach().numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde = MHD()\n",
    "t = torch.ones(resolution, requires_grad=True)\n",
    "x = torch.zeros(resolution, requires_grad=True)\n",
    "gradients(conserved(pde(t, x)),t)\n",
    "#output.grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=pde(t,x).t()\n",
    "trial=torch.einsum('i,j->ij',pp[0], pp[1])\n",
    "trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=pde(t,x).t()\n",
    "pp[0]\n",
    "\n",
    "torch.stack((pp[0],pp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.as_tensor(list(map(initial_condition, x))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((pde(t, x) - torch.as_tensor(list(map(initial_condition, x))))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RMHD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
